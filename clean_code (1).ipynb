{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f992c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: (199, 5) target: (199,)\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- GridSearchCV for Random Forest ---\n",
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 106\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- GridSearchCV for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    102\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m    103\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mpipeline, param_grid\u001b[38;5;241m=\u001b[39mparams, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    104\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv_list_grid, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    105\u001b[0m )\n\u001b[1;32m--> 106\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest params for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\eski_jupyter\\lib\\site-packages\\sklearn\\base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m ):\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\eski_jupyter\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1046\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1047\u001b[0m     )\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1051\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\eski_jupyter\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1605\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\eski_jupyter\\lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    993\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    994\u001b[0m         )\n\u001b[0;32m    995\u001b[0m     )\n\u001b[1;32m--> 997\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1020\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\eski_jupyter\\lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[0;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     75\u001b[0m     (\n\u001b[0;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     81\u001b[0m )\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\eski_jupyter\\lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\eski_jupyter\\lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\eski_jupyter\\lib\\site-packages\\joblib\\parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[0;32m   1799\u001b[0m     ):\n\u001b[1;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# FINAL SCRIPT \n",
    "import numpy as np, pandas as pd, time, tempfile, os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "#  1) composition-only dataset \n",
    "DATA_PATH = r\"C:\\Users\\kagan\\Downloads\\COMPO_ONLY_model_ready_with_dSref_n0913.csv\"  # <- kendi yoluna göre değiştir\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "y = df[\"dS_ref\"].astype(np.float32)\n",
    "X = df.drop(columns=[\"dS_ref\"]).astype(np.float32)\n",
    "\n",
    "print(\"Dataset loaded:\", X.shape, \"target:\", y.shape)\n",
    "print(\"-\"*60)\n",
    "\n",
    "#  2) CV splitters: quantile-stratified (stabil R² için) \n",
    "# y'yi 5 dilime böl, aynı dağılımı her fold'a taşı\n",
    "bins = pd.qcut(y, q=5, labels=False, duplicates='drop')\n",
    "cv_for_grid = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_for_eval = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_list_grid = list(cv_for_grid.split(X, bins))\n",
    "cv_list_eval = list(cv_for_eval.split(X, bins))\n",
    "\n",
    "#  3) Pipelines (Scaler: sadece SVR). Cache açık.\n",
    "cache_dir = tempfile.mkdtemp(prefix=\"sk_cache_\")\n",
    "\n",
    "pipe_rfr = Pipeline([('regressor', RandomForestRegressor(random_state=42, n_jobs=-1))],\n",
    "                    memory=cache_dir)\n",
    "\n",
    "gbr_estimator = GradientBoostingRegressor(\n",
    "    loss=\"huber\", validation_fraction=0.1, n_iter_no_change=10, tol=1e-4, random_state=42\n",
    ")\n",
    "pipe_gbr = Pipeline([('regressor', gbr_estimator)], memory=cache_dir)\n",
    "\n",
    "pipe_xgb = Pipeline([('regressor', xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                                                    random_state=42, n_jobs=-1))],\n",
    "                    memory=cache_dir)\n",
    "\n",
    "pipe_lgbm = Pipeline([('regressor', lgb.LGBMRegressor(random_state=42, n_jobs=-1))],\n",
    "                     memory=cache_dir)\n",
    "\n",
    "pipe_svr = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('regressor', SVR(kernel='rbf'))],\n",
    "                    memory=cache_dir)\n",
    "\n",
    "# 4) Hyperparameter grids  \n",
    "param_grid_rfr = {\n",
    "    'regressor__n_estimators': [100, 200, 400, 800],\n",
    "    'regressor__max_depth': [10, 20, 30, None],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 5],\n",
    "    'regressor__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "param_grid_gbr = {\n",
    "    'regressor__n_estimators': [100, 200, 400, 600, 800, 1000],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__max_depth': [3, 5, 7, 9],\n",
    "    'regressor__subsample': [0.6, 1.0],\n",
    "    'regressor__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "param_grid_xgb = {\n",
    "    'regressor__n_estimators': [200, 400, 600, 800],\n",
    "    'regressor__max_depth': [3, 5, 7, 9],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__subsample': [0.6, 1.0],\n",
    "    'regressor__colsample_bytree': [0.6, 1.0]\n",
    "}\n",
    "param_grid_lgbm_extended = {\n",
    "    'regressor__n_estimators': [200, 400, 600, 800],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__num_leaves': [20, 31, 40],\n",
    "    'regressor__max_depth': [5, 10, -1],\n",
    "    'regressor__subsample': [0.8, 1.0],\n",
    "    'regressor__colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "param_grid_svr_extended = {\n",
    "    'regressor__C': [1, 10, 100, 500],\n",
    "    'regressor__gamma': [0.01, 0.1, 'scale']\n",
    "}\n",
    "\n",
    "models_to_run = {\n",
    "    'Random Forest': (pipe_rfr, param_grid_rfr),\n",
    "    'Gradient Boosting': (pipe_gbr, param_grid_gbr),\n",
    "    'XGBoost': (pipe_xgb, param_grid_xgb),\n",
    "    'LightGBM': (pipe_lgbm, param_grid_lgbm_extended),\n",
    "    'SVR': (pipe_svr, param_grid_svr_extended)\n",
    "}\n",
    "\n",
    "# 5) Two-stage evaluation  \n",
    "all_results = {}\n",
    "scoring_metrics = ['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error']\n",
    "\n",
    "for model_name, (pipeline, params) in models_to_run.items():\n",
    "    t0 = time.time()\n",
    "    print(f\"\\n--- GridSearchCV for {model_name} ---\")\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline, param_grid=params, scoring='r2',\n",
    "        cv=cv_list_grid, n_jobs=-1, verbose=1\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"Best params for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "    print(f\"--- cross_validate for {model_name} ---\")\n",
    "    cv_scores = cross_validate(best_model, X, y,\n",
    "                               scoring=scoring_metrics,\n",
    "                               cv=cv_list_eval, n_jobs=-1,\n",
    "                               return_train_score=False)\n",
    "    t1 = time.time()\n",
    "\n",
    "    all_results[model_name] = {\n",
    "        'mean_r2': cv_scores['test_r2'].mean(),\n",
    "        'std_r2': cv_scores['test_r2'].std(),\n",
    "        'mean_rmse': np.sqrt(-cv_scores['test_neg_mean_squared_error']).mean(),\n",
    "        'std_rmse': np.sqrt(-cv_scores['test_neg_mean_squared_error']).std(),\n",
    "        'mean_mae': -cv_scores['test_neg_mean_absolute_error'].mean(),\n",
    "        'std_mae': cv_scores['test_neg_mean_absolute_error'].std(),\n",
    "        'best_estimator': best_model,\n",
    "        'time_taken': t1 - t0\n",
    "    }\n",
    "    print(f\"Done {model_name} in {t1-t0:.1f}s\")\n",
    "\n",
    "#  6) Report \n",
    "final_df = pd.DataFrame(all_results).T.sort_values(by='mean_r2', ascending=False)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL CONSOLIDATED MODEL COMPARISON (grids unchanged)\")\n",
    "print(\"=\"*60)\n",
    "print(final_df[['mean_r2','std_r2','mean_rmse','std_rmse','mean_mae','std_mae']].round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANA KODUN EN SONUNA EKLENECEK KAYDETME BLOĞU\n",
    "import pickle\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Tüm model sonuçları 'final_model_results.pkl' dosyasına kaydediliyor...\")\n",
    "\n",
    "with open('final_model_results.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "print(\"Kaydetme işlemi başarıyla tamamlandı.\")\n",
    "print(\"Artık bu sonuçları istediğiniz zaman yeniden yükleyebilirsiniz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5940cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETAYLI ANALİZ BLOĞU\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# İSINSTANCE kontrolleri için:\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Gerekli değişkenlerin ortamda olduğunu kontrol et \n",
    "missing = []\n",
    "for name in [\"all_results\", \"X\", \"y\"]:\n",
    "    if name not in globals():\n",
    "        missing.append(name)\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Gerekli değişken(ler) eksik: {missing}. Lütfen önce ana değerlendirme kodunu çalıştırın.\")\n",
    "\n",
    "print(\"Ana değerlendirme sonuçları ve veri bulundu.\")\n",
    "\n",
    "# Her model için analizleri ve grafikleri oluştur \n",
    "for model_name, result_data in all_results.items():\n",
    "\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(f\"ANALYSIS FOR MODEL: {model_name}\")\n",
    "    print(\"=\"*90)\n",
    "\n",
    "    # En iyi pipeline ve bileşenlerini al\n",
    "    best_model_pipeline = result_data['best_estimator']\n",
    "\n",
    "    # scaler opsiyonel olabilir (ağaçlarda yok)\n",
    "    scaler = best_model_pipeline.named_steps.get('scaler', None)\n",
    "    regressor = best_model_pipeline.named_steps.get('regressor', best_model_pipeline)\n",
    "\n",
    "    # X'i SHAP/Permutation için hazırlama\n",
    "    if scaler is not None:\n",
    "        # SVR vb. için ölçekli X\n",
    "        try:\n",
    "            X_for_explain = pd.DataFrame(scaler.transform(X), columns=X.columns)\n",
    "        except Exception as e:\n",
    "            print(f\"[Uyarı] Scaler.transform sırasında hata: {e}. Ham X kullanılacak.\")\n",
    "            X_for_explain = X.copy()\n",
    "    else:\n",
    "        # Ağaç modellerinde ham X\n",
    "        X_for_explain = X.copy()\n",
    "\n",
    "    # 1) Standard Feature Importance (ağaç modellerinde mevcut) \n",
    "    print(f\"\\n--- 1) Standard Feature Importance for {model_name} ---\")\n",
    "    if hasattr(regressor, \"feature_importances_\"):\n",
    "        try:\n",
    "            importances = regressor.feature_importances_\n",
    "            fi_df = pd.DataFrame({\"feature\": X.columns, \"importance\": importances}) \\\n",
    "                        .sort_values(\"importance\", ascending=False)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.barplot(x='importance', y='feature', data=fi_df, palette='viridis')\n",
    "            plt.title(f'Standard Feature Importance ({model_name})', fontsize=16)\n",
    "            plt.xlabel('Importance Score', fontsize=12)\n",
    "            plt.ylabel('Features', fontsize=12)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"[Atlandı] feature_importances_ çizimi sırasında hata: {e}\")\n",
    "    else:\n",
    "        print(f\"NOTE: '{model_name}' modelinde 'feature_importances_' özelliği yok (ör. SVR).\")\n",
    "\n",
    "    # 2) Permutation Feature Importance (pipeline üzerinde, R² düşüşü) \n",
    "    print(f\"\\n--- 2) Permutation Importance for {model_name} ---\")\n",
    "    try:\n",
    "        perm = permutation_importance(\n",
    "            best_model_pipeline, X, y,\n",
    "            n_repeats=10, random_state=42, n_jobs=-1, scoring='r2'\n",
    "        )\n",
    "        sorted_idx = perm.importances_mean.argsort()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        ax.boxplot(perm.importances[sorted_idx].T,\n",
    "                   vert=False, labels=X.columns[sorted_idx])\n",
    "        ax.set_title(f\"Permutation Importance ({model_name})\", fontsize=16)\n",
    "        ax.set_xlabel(\"Performance Decrease (R²)\", fontsize=12)\n",
    "        ax.set_ylabel(\"Features\", fontsize=12)\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.4)\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[Atlandı] Permutation importance sırasında hata: {e}\")\n",
    "\n",
    "    # 3) SHAP Analizi \n",
    "    print(f\"\\n--- 3) SHAP Analysis for {model_name} ---\")\n",
    "    try:\n",
    "        # Ağaç modelleri: TreeExplainer\n",
    "        if isinstance(regressor, (RandomForestRegressor,\n",
    "                                  GradientBoostingRegressor,\n",
    "                                  xgb.XGBRegressor,\n",
    "                                  lgb.LGBMRegressor)):\n",
    "            explainer = shap.TreeExplainer(regressor)\n",
    "            shap_values = explainer.shap_values(X_for_explain)\n",
    "            plt.figure()\n",
    "            shap.summary_plot(shap_values, X_for_explain, show=False)\n",
    "            plt.gcf().suptitle(f\"SHAP Summary Plot ({model_name})\", fontsize=16)\n",
    "            plt.show()\n",
    "\n",
    "        # SVR ve diğer modeller: KernelExplainer (pipeline.predict ile)\n",
    "        elif isinstance(regressor, SVR):\n",
    "            print(\"NOTE: Using KernelExplainer (daha yavaş). 100 örnek arka plan kullanılacak.\")\n",
    "            background = shap.sample(X_for_explain, 100, random_state=42)\n",
    "            explainer = shap.KernelExplainer(best_model_pipeline.predict, background)\n",
    "            shap_values = explainer.shap_values(X_for_explain, nsamples=100)  # hız için sınır\n",
    "            plt.figure()\n",
    "            shap.summary_plot(shap_values, X_for_explain, show=False)\n",
    "            plt.gcf().suptitle(f\"SHAP Summary Plot ({model_name})\", fontsize=16)\n",
    "            plt.show()\n",
    "        else:\n",
    "            # Diğer olası regressor tipleri için genel Explainer (yavaş olabilir)\n",
    "            print(\"NOTE: Generic shap.Explainer kullanılıyor (yavaş olabilir).\")\n",
    "            explainer = shap.Explainer(best_model_pipeline.predict, X_for_explain)\n",
    "            shap_values = explainer(X_for_explain)\n",
    "            plt.figure()\n",
    "            shap.summary_plot(shap_values, X_for_explain, show=False)\n",
    "            plt.gcf().suptitle(f\"SHAP Summary Plot ({model_name})\", fontsize=16)\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Atlandı] SHAP analizi sırasında hata: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2e55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Final Comparison Plots (robust, annotated, matplotlib-only) \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Model sırası \n",
    "order = ['Gradient Boosting', 'XGBoost', 'Random Forest', 'LightGBM', 'SVR']\n",
    "df_plot = final_df.loc[order].copy()\n",
    "\n",
    "# 2) Sayısal güvenlik\n",
    "for c in ['mean_r2','std_r2','mean_rmse','std_rmse','mean_mae','std_mae']:\n",
    "    df_plot[c] = pd.to_numeric(df_plot[c], errors='coerce')\n",
    "\n",
    "# 3) Ortak renk paleti (viridis)\n",
    "colors = plt.cm.viridis(np.linspace(0.12, 0.88, len(order)))\n",
    "\n",
    "# 4) Figür\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 7))\n",
    "fig.suptitle('Model Performance Comparison (10-fold Cross-Validation)', fontsize=22, y=1.03)\n",
    "\n",
    "# Panel 1: R² \n",
    "axes[0].bar(order, df_plot['mean_r2'], yerr=df_plot['std_r2'],\n",
    "            color=colors, edgecolor='black', capsize=8)\n",
    "axes[0].set_title('Mean R² Score (Higher is better)', fontsize=16)\n",
    "axes[0].set_ylabel('R² Score', fontsize=14)\n",
    "axes[0].set_ylim(bottom=0)\n",
    "\n",
    "#  Panel 2: RMSE \n",
    "axes[1].bar(order, df_plot['mean_rmse'], yerr=df_plot['std_rmse'],\n",
    "            color=colors, edgecolor='black', capsize=8)\n",
    "axes[1].set_title('Mean RMSE (Lower is better)', fontsize=16)\n",
    "axes[1].set_ylabel('RMSE', fontsize=14)\n",
    "axes[1].set_ylim(bottom=0)\n",
    "\n",
    "# Panel 3: MAE \n",
    "axes[2].bar(order, df_plot['mean_mae'], yerr=df_plot['std_mae'],\n",
    "            color=colors, edgecolor='black', capsize=8)\n",
    "axes[2].set_title('Mean MAE (Lower is better)', fontsize=16)\n",
    "axes[2].set_ylabel('MAE', fontsize=14)\n",
    "axes[2].set_ylim(bottom=0)\n",
    "\n",
    "# Ortak stil\n",
    "for ax in axes:\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=12)\n",
    "    ax.set_xlabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # suptitle için boşluk\n",
    "plt.savefig('model_performance_comparison_preferred.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# İSTATİSTİKSEL ANALİZ ve TASARIM KURALI DOĞRULAMA \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#  Yardımcı fonksiyonlar \n",
    "def numeric_cols(df):\n",
    "    return df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "def bootstrap_ci(stat_fn, y_true, y_pred, n_boot=5000, alpha=0.05, rng=42):\n",
    "    \"\"\"İkili sınıflandırma metrikleri için bootstrap GA (bias-corrected değil, hızlı).\"\"\"\n",
    "    rs = np.random.RandomState(rng)\n",
    "    stats = []\n",
    "    n = len(y_true)\n",
    "    idx = np.arange(n)\n",
    "    for _ in range(n_boot):\n",
    "        b = rs.choice(idx, size=n, replace=True)\n",
    "        stats.append(stat_fn(y_true[b], y_pred[b]))\n",
    "    lo = np.percentile(stats, 100*alpha/2)\n",
    "    hi = np.percentile(stats, 100*(1-alpha/2))\n",
    "    return float(np.mean(stats)), float(lo), float(hi)\n",
    "\n",
    "def metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    acc = (tp+tn) / cm.sum()\n",
    "    prec = tp / (tp+fp) if (tp+fp)>0 else 0.0\n",
    "    rec  = tp / (tp+fn) if (tp+fn)>0 else 0.0  # TPR / sensitivity\n",
    "    spec = tn / (tn+fp) if (tn+fp)>0 else 0.0 # TNR / specificity\n",
    "    f1   = (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    # Matthews correlation\n",
    "    denom = np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    mcc = ((tp*tn - fp*fn)/denom) if denom>0 else 0.0\n",
    "    cov = (tp+fp)/cm.sum()  # kuralın \"High\" dediği pay (coverage)\n",
    "    return dict(accuracy=acc, precision=prec, recall=rec, specificity=spec, f1=f1, mcc=mcc, coverage=cov)\n",
    "\n",
    "def plot_confusion(cm, labels=('Actual Low','Actual High'), preds=('Predicted Low','Predicted High'), title='Confusion Matrix'):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    totals = cm.sum()\n",
    "    pct = cm / totals * 100.0\n",
    "    annot = np.array([\n",
    "        [f\"TN\\n{tn}\\n{pct[0,0]:.1f}%\", f\"FP\\n{fp}\\n{pct[0,1]:.1f}%\"],\n",
    "        [f\"FN\\n{fn}\\n{pct[1,0]:.1f}%\", f\"TP\\n{tp}\\n{pct[1,1]:.1f}%\"]\n",
    "    ])\n",
    "    plt.figure(figsize=(7,6))\n",
    "    ax = sns.heatmap(cm, annot=annot, fmt='', cmap='Blues', cbar=False,\n",
    "                     xticklabels=preds, yticklabels=labels, linewidths=1, linecolor='black')\n",
    "    ax.set_xlabel('Predicted by Rules', fontsize=12)\n",
    "    ax.set_ylabel('Actual Performance', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 1) Veriyi yükle \n",
    "print(\"Loading dataset for statistical analysis...\")\n",
    "DATA_PATH = r\"C:\\Users\\kagan\\Downloads\\COMPO_ONLY_dataset.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns.\")\n",
    "\n",
    "#  2) Korelasyon ısı haritaları \n",
    "print(\"\\nGenerating Correlation Heatmaps (numeric columns only)...\")\n",
    "num_cols = numeric_cols(df)\n",
    "pearson_corr = df[num_cols].corr(method='pearson')\n",
    "spearman_corr = df[num_cols].corr(method='spearman')\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(pearson_corr, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=.5)\n",
    "plt.title('Pearson Correlation (numeric features)', fontsize=16)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(spearman_corr, annot=True, fmt=\".2f\", cmap='viridis', linewidths=.5)\n",
    "plt.title('Spearman Correlation (numeric features)', fontsize=16)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "#  3) Gerçek performans etiketleri \n",
    "performance_threshold = 9.0\n",
    "df['Actual Performance'] = np.where(df['dS'] >= performance_threshold, 1, 0)  # 1=High, 0=Low\n",
    "print(\"\\nCounts:\", df['Actual Performance'].value_counts().to_dict())\n",
    "\n",
    "#  4) Özellik dağılımları (kutu grafikleri) \n",
    "features_to_compare = [c for c in ['VEC','dHmix','dSmix'] if c in df.columns]\n",
    "print(\"\\nGenerating feature distribution plots...\")\n",
    "for feature in features_to_compare:\n",
    "    plt.figure(figsize=(9,6))\n",
    "    sns.boxplot(x='Actual Performance', y=feature, data=df, palette='viridis',\n",
    "                order=[0,1])\n",
    "    plt.title(f'Distribution of {feature} by Performance Group', fontsize=14)\n",
    "    plt.xlabel('Actual Performance (0=Low, 1=High)', fontsize=12)\n",
    "    plt.ylabel(feature, fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "#  5) Tasarım kuralı ve Confusion Matrix \n",
    "print(\"\\nEvaluating fixed compositional design rules...\")\n",
    "\n",
    "#  kural sınırı yaz \n",
    "rule_VEC  = (df['VEC']  >= 2.40) & (df['VEC']  <= 7.30) if 'VEC'  in df.columns else True\n",
    "rule_dSmix= (df['dSmix']>=11.50) & (df['dSmix']<=19.10) if 'dSmix'in df.columns else True\n",
    "rule_dHmix= (df['dHmix']>=-46.5) & (df['dHmix']<=-27.6)  if 'dHmix' in df.columns else True\n",
    "\n",
    "rule_mask = rule_VEC & rule_dSmix & rule_dHmix\n",
    "df['Predicted Performance'] = np.where(rule_mask, 1, 0)\n",
    "\n",
    "# Confusion matrix (label order: [Low, High] = [0,1])\n",
    "cm = confusion_matrix(df['Actual Performance'], df['Predicted Performance'], labels=[0,1])\n",
    "plot_confusion(cm, labels=('Actual Low','Actual High'), preds=('Predicted Low','Predicted High'),\n",
    "               title='Confusion Matrix for Compositional Design Rules')\n",
    "\n",
    "#  6) Metrikler + Bootstrap GA \n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "base_metrics = metrics_from_cm(cm)\n",
    "print(\"\\nPoint estimates:\")\n",
    "for k,v in base_metrics.items():\n",
    "    print(f\"  {k:>11s}: {v:.3f}\")\n",
    "\n",
    "# Bootstrap GA (accuracy, precision, recall, specificity, F1, MCC, coverage)\n",
    "y_true = df['Actual Performance'].to_numpy()\n",
    "y_pred = df['Predicted Performance'].to_numpy()\n",
    "\n",
    "def stat_acc(y, yhat):  return ((y==yhat).mean())\n",
    "def stat_prec(y, yhat): \n",
    "    tp = ((y==1)&(yhat==1)).sum(); fp = ((y==0)&(yhat==1)).sum()\n",
    "    return tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "def stat_rec(y, yhat):\n",
    "    tp = ((y==1)&(yhat==1)).sum(); fn = ((y==1)&(yhat==0)).sum()\n",
    "    return tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "def stat_spec(y, yhat):\n",
    "    tn = ((y==0)&(yhat==0)).sum(); fp = ((y==0)&(yhat==1)).sum()\n",
    "    return tn/(tn+fp) if (tn+fp)>0 else 0.0\n",
    "def stat_f1(y, yhat):\n",
    "    p = stat_prec(y,yhat); r = stat_rec(y,yhat); \n",
    "    return (2*p*r)/(p+r) if (p+r)>0 else 0.0\n",
    "def stat_mcc(y, yhat):\n",
    "    tp = ((y==1)&(yhat==1)).sum(); tn = ((y==0)&(yhat==0)).sum()\n",
    "    fp = ((y==0)&(yhat==1)).sum(); fn = ((y==1)&(yhat==0)).sum()\n",
    "    denom = np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    return ((tp*tn - fp*fn)/denom) if denom>0 else 0.0\n",
    "def stat_cov(y, yhat): return (yhat==1).mean()\n",
    "\n",
    "stats = {\n",
    "    \"accuracy\":   stat_acc,\n",
    "    \"precision\":  stat_prec,\n",
    "    \"recall_TPR\": stat_rec,\n",
    "    \"specificity\":stat_spec,\n",
    "    \"f1\":         stat_f1,\n",
    "    \"mcc\":        stat_mcc,\n",
    "    \"coverage\":   stat_cov\n",
    "}\n",
    "\n",
    "print(\"\\nBootstrap 95% CI (5000 resamples):\")\n",
    "for name, fn in stats.items():\n",
    "    mean, lo, hi = bootstrap_ci(lambda yt, yp: fn(yt, yp), y_true, y_pred, n_boot=5000, alpha=0.05, rng=123)\n",
    "    print(f\"  {name:>11s}: {mean:.3f}  (95% CI: {lo:.3f}–{hi:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
