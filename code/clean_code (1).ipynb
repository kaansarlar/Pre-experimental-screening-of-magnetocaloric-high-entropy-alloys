{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b5b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL SCRIPT \n",
    "import numpy as np, pandas as pd, time, tempfile, os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "#  1) composition-only dataset \n",
    "DATA_PATH = r\"C:\\Users\\kagan\\Downloads\\COMPO_ONLY_model_ready_with_dSref_n0913.csv\"  # <- kendi yoluna göre değiştir\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "y = df[\"dS_ref\"].astype(np.float32)\n",
    "X = df.drop(columns=[\"dS_ref\"]).astype(np.float32)\n",
    "\n",
    "print(\"Dataset loaded:\", X.shape, \"target:\", y.shape)\n",
    "print(\"-\"*60)\n",
    "\n",
    "#  2) CV splitters: quantile-stratified (stabil R² için) \n",
    "# y'yi 5 dilime böl, aynı dağılımı her fold'a taşı\n",
    "bins = pd.qcut(y, q=5, labels=False, duplicates='drop')\n",
    "cv_for_grid = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_for_eval = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_list_grid = list(cv_for_grid.split(X, bins))\n",
    "cv_list_eval = list(cv_for_eval.split(X, bins))\n",
    "\n",
    "#  3) Pipelines (Scaler: sadece SVR). Cache açık.\n",
    "cache_dir = tempfile.mkdtemp(prefix=\"sk_cache_\")\n",
    "\n",
    "pipe_rfr = Pipeline([('regressor', RandomForestRegressor(random_state=42, n_jobs=-1))],\n",
    "                    memory=cache_dir)\n",
    "\n",
    "gbr_estimator = GradientBoostingRegressor(\n",
    "    loss=\"huber\", validation_fraction=0.1, n_iter_no_change=10, tol=1e-4, random_state=42\n",
    ")\n",
    "pipe_gbr = Pipeline([('regressor', gbr_estimator)], memory=cache_dir)\n",
    "\n",
    "pipe_xgb = Pipeline([('regressor', xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                                                    random_state=42, n_jobs=-1))],\n",
    "                    memory=cache_dir)\n",
    "\n",
    "pipe_lgbm = Pipeline([('regressor', lgb.LGBMRegressor(random_state=42, n_jobs=-1))],\n",
    "                     memory=cache_dir)\n",
    "\n",
    "pipe_svr = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('regressor', SVR(kernel='rbf'))],\n",
    "                    memory=cache_dir)\n",
    "\n",
    "# 4) Hyperparameter grids  \n",
    "param_grid_rfr = {\n",
    "    'regressor__n_estimators': [100, 200, 400, 800],\n",
    "    'regressor__max_depth': [10, 20, 30, None],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 5],\n",
    "    'regressor__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "param_grid_gbr = {\n",
    "    'regressor__n_estimators': [100, 200, 400, 600, 800, 1000],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__max_depth': [3, 5, 7, 9],\n",
    "    'regressor__subsample': [0.6, 1.0],\n",
    "    'regressor__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "param_grid_xgb = {\n",
    "    'regressor__n_estimators': [200, 400, 600, 800],\n",
    "    'regressor__max_depth': [3, 5, 7, 9],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__subsample': [0.6, 1.0],\n",
    "    'regressor__colsample_bytree': [0.6, 1.0]\n",
    "}\n",
    "param_grid_lgbm_extended = {\n",
    "    'regressor__n_estimators': [200, 400, 600, 800],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__num_leaves': [20, 31, 40],\n",
    "    'regressor__max_depth': [5, 10, -1],\n",
    "    'regressor__subsample': [0.8, 1.0],\n",
    "    'regressor__colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "param_grid_svr_extended = {\n",
    "    'regressor__C': [1, 10, 100, 500],\n",
    "    'regressor__gamma': [0.01, 0.1, 'scale']\n",
    "}\n",
    "\n",
    "models_to_run = {\n",
    "    'Random Forest': (pipe_rfr, param_grid_rfr),\n",
    "    'Gradient Boosting': (pipe_gbr, param_grid_gbr),\n",
    "    'XGBoost': (pipe_xgb, param_grid_xgb),\n",
    "    'LightGBM': (pipe_lgbm, param_grid_lgbm_extended),\n",
    "    'SVR': (pipe_svr, param_grid_svr_extended)\n",
    "}\n",
    "\n",
    "# 5) Two-stage evaluation  \n",
    "all_results = {}\n",
    "scoring_metrics = ['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error']\n",
    "\n",
    "for model_name, (pipeline, params) in models_to_run.items():\n",
    "    t0 = time.time()\n",
    "    print(f\"\\n--- GridSearchCV for {model_name} ---\")\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline, param_grid=params, scoring='r2',\n",
    "        cv=cv_list_grid, n_jobs=-1, verbose=1\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"Best params for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "    print(f\"--- cross_validate for {model_name} ---\")\n",
    "    cv_scores = cross_validate(best_model, X, y,\n",
    "                               scoring=scoring_metrics,\n",
    "                               cv=cv_list_eval, n_jobs=-1,\n",
    "                               return_train_score=False)\n",
    "    t1 = time.time()\n",
    "\n",
    "    all_results[model_name] = {\n",
    "        'mean_r2': cv_scores['test_r2'].mean(),\n",
    "        'std_r2': cv_scores['test_r2'].std(),\n",
    "        'mean_rmse': np.sqrt(-cv_scores['test_neg_mean_squared_error']).mean(),\n",
    "        'std_rmse': np.sqrt(-cv_scores['test_neg_mean_squared_error']).std(),\n",
    "        'mean_mae': -cv_scores['test_neg_mean_absolute_error'].mean(),\n",
    "        'std_mae': cv_scores['test_neg_mean_absolute_error'].std(),\n",
    "        'best_estimator': best_model,\n",
    "        'time_taken': t1 - t0\n",
    "    }\n",
    "    print(f\"Done {model_name} in {t1-t0:.1f}s\")\n",
    "\n",
    "#  6) Report \n",
    "final_df = pd.DataFrame(all_results).T.sort_values(by='mean_r2', ascending=False)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL CONSOLIDATED MODEL COMPARISON (grids unchanged)\")\n",
    "print(\"=\"*60)\n",
    "print(final_df[['mean_r2','std_r2','mean_rmse','std_rmse','mean_mae','std_mae']].round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANA KODUN EN SONUNA EKLENECEK KAYDETME BLOĞU\n",
    "import pickle\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Tüm model sonuçları 'final_model_results.pkl' dosyasına kaydediliyor...\")\n",
    "\n",
    "with open('final_model_results.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "print(\"Kaydetme işlemi başarıyla tamamlandı.\")\n",
    "print(\"Artık bu sonuçları istediğiniz zaman yeniden yükleyebilirsiniz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5940cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETAYLI ANALİZ BLOĞU\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# İSINSTANCE kontrolleri için:\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Gerekli değişkenlerin ortamda olduğunu kontrol et \n",
    "missing = []\n",
    "for name in [\"all_results\", \"X\", \"y\"]:\n",
    "    if name not in globals():\n",
    "        missing.append(name)\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Gerekli değişken(ler) eksik: {missing}. Lütfen önce ana değerlendirme kodunu çalıştırın.\")\n",
    "\n",
    "print(\"Ana değerlendirme sonuçları ve veri bulundu.\")\n",
    "\n",
    "# Her model için analizleri ve grafikleri oluştur \n",
    "for model_name, result_data in all_results.items():\n",
    "\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(f\"ANALYSIS FOR MODEL: {model_name}\")\n",
    "    print(\"=\"*90)\n",
    "\n",
    "    # En iyi pipeline ve bileşenlerini al\n",
    "    best_model_pipeline = result_data['best_estimator']\n",
    "\n",
    "    # scaler opsiyonel olabilir (ağaçlarda yok)\n",
    "    scaler = best_model_pipeline.named_steps.get('scaler', None)\n",
    "    regressor = best_model_pipeline.named_steps.get('regressor', best_model_pipeline)\n",
    "\n",
    "    # X'i SHAP/Permutation için hazırlama\n",
    "    if scaler is not None:\n",
    "        # SVR vb. için ölçekli X\n",
    "        try:\n",
    "            X_for_explain = pd.DataFrame(scaler.transform(X), columns=X.columns)\n",
    "        except Exception as e:\n",
    "            print(f\"[Uyarı] Scaler.transform sırasında hata: {e}. Ham X kullanılacak.\")\n",
    "            X_for_explain = X.copy()\n",
    "    else:\n",
    "        # Ağaç modellerinde ham X\n",
    "        X_for_explain = X.copy()\n",
    "\n",
    "    # 1) Standard Feature Importance (ağaç modellerinde mevcut) \n",
    "    print(f\"\\n--- 1) Standard Feature Importance for {model_name} ---\")\n",
    "    if hasattr(regressor, \"feature_importances_\"):\n",
    "        try:\n",
    "            importances = regressor.feature_importances_\n",
    "            fi_df = pd.DataFrame({\"feature\": X.columns, \"importance\": importances}) \\\n",
    "                        .sort_values(\"importance\", ascending=False)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.barplot(x='importance', y='feature', data=fi_df, palette='viridis')\n",
    "            plt.title(f'Standard Feature Importance ({model_name})', fontsize=16)\n",
    "            plt.xlabel('Importance Score', fontsize=12)\n",
    "            plt.ylabel('Features', fontsize=12)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"[Atlandı] feature_importances_ çizimi sırasında hata: {e}\")\n",
    "    else:\n",
    "        print(f\"NOTE: '{model_name}' modelinde 'feature_importances_' özelliği yok (ör. SVR).\")\n",
    "\n",
    "    # 2) Permutation Feature Importance (pipeline üzerinde, R² düşüşü) \n",
    "    print(f\"\\n--- 2) Permutation Importance for {model_name} ---\")\n",
    "    try:\n",
    "        perm = permutation_importance(\n",
    "            best_model_pipeline, X, y,\n",
    "            n_repeats=10, random_state=42, n_jobs=-1, scoring='r2'\n",
    "        )\n",
    "        sorted_idx = perm.importances_mean.argsort()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        ax.boxplot(perm.importances[sorted_idx].T,\n",
    "                   vert=False, labels=X.columns[sorted_idx])\n",
    "        ax.set_title(f\"Permutation Importance ({model_name})\", fontsize=16)\n",
    "        ax.set_xlabel(\"Performance Decrease (R²)\", fontsize=12)\n",
    "        ax.set_ylabel(\"Features\", fontsize=12)\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.4)\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[Atlandı] Permutation importance sırasında hata: {e}\")\n",
    "\n",
    "    # 3) SHAP Analizi \n",
    "    print(f\"\\n--- 3) SHAP Analysis for {model_name} ---\")\n",
    "    try:\n",
    "        # Ağaç modelleri: TreeExplainer\n",
    "        if isinstance(regressor, (RandomForestRegressor,\n",
    "                                  GradientBoostingRegressor,\n",
    "                                  xgb.XGBRegressor,\n",
    "                                  lgb.LGBMRegressor)):\n",
    "            explainer = shap.TreeExplainer(regressor)\n",
    "            shap_values = explainer.shap_values(X_for_explain)\n",
    "            plt.figure()\n",
    "            shap.summary_plot(shap_values, X_for_explain, show=False)\n",
    "            plt.gcf().suptitle(f\"SHAP Summary Plot ({model_name})\", fontsize=16)\n",
    "            plt.show()\n",
    "\n",
    "        # SVR ve diğer modeller: KernelExplainer (pipeline.predict ile)\n",
    "        elif isinstance(regressor, SVR):\n",
    "            print(\"NOTE: Using KernelExplainer (daha yavaş). 100 örnek arka plan kullanılacak.\")\n",
    "            background = shap.sample(X_for_explain, 100, random_state=42)\n",
    "            explainer = shap.KernelExplainer(best_model_pipeline.predict, background)\n",
    "            shap_values = explainer.shap_values(X_for_explain, nsamples=100)  # hız için sınır\n",
    "            plt.figure()\n",
    "            shap.summary_plot(shap_values, X_for_explain, show=False)\n",
    "            plt.gcf().suptitle(f\"SHAP Summary Plot ({model_name})\", fontsize=16)\n",
    "            plt.show()\n",
    "        else:\n",
    "            # Diğer olası regressor tipleri için genel Explainer (yavaş olabilir)\n",
    "            print(\"NOTE: Generic shap.Explainer kullanılıyor (yavaş olabilir).\")\n",
    "            explainer = shap.Explainer(best_model_pipeline.predict, X_for_explain)\n",
    "            shap_values = explainer(X_for_explain)\n",
    "            plt.figure()\n",
    "            shap.summary_plot(shap_values, X_for_explain, show=False)\n",
    "            plt.gcf().suptitle(f\"SHAP Summary Plot ({model_name})\", fontsize=16)\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Atlandı] SHAP analizi sırasında hata: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2e55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Final Comparison Plots (robust, annotated, matplotlib-only) \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Model sırası \n",
    "order = ['Gradient Boosting', 'XGBoost', 'Random Forest', 'LightGBM', 'SVR']\n",
    "df_plot = final_df.loc[order].copy()\n",
    "\n",
    "# 2) Sayısal güvenlik\n",
    "for c in ['mean_r2','std_r2','mean_rmse','std_rmse','mean_mae','std_mae']:\n",
    "    df_plot[c] = pd.to_numeric(df_plot[c], errors='coerce')\n",
    "\n",
    "# 3) Ortak renk paleti (viridis)\n",
    "colors = plt.cm.viridis(np.linspace(0.12, 0.88, len(order)))\n",
    "\n",
    "# 4) Figür\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 7))\n",
    "fig.suptitle('Model Performance Comparison (10-fold Cross-Validation)', fontsize=22, y=1.03)\n",
    "\n",
    "# Panel 1: R² \n",
    "axes[0].bar(order, df_plot['mean_r2'], yerr=df_plot['std_r2'],\n",
    "            color=colors, edgecolor='black', capsize=8)\n",
    "axes[0].set_title('Mean R² Score (Higher is better)', fontsize=16)\n",
    "axes[0].set_ylabel('R² Score', fontsize=14)\n",
    "axes[0].set_ylim(bottom=0)\n",
    "\n",
    "#  Panel 2: RMSE \n",
    "axes[1].bar(order, df_plot['mean_rmse'], yerr=df_plot['std_rmse'],\n",
    "            color=colors, edgecolor='black', capsize=8)\n",
    "axes[1].set_title('Mean RMSE (Lower is better)', fontsize=16)\n",
    "axes[1].set_ylabel('RMSE', fontsize=14)\n",
    "axes[1].set_ylim(bottom=0)\n",
    "\n",
    "# Panel 3: MAE \n",
    "axes[2].bar(order, df_plot['mean_mae'], yerr=df_plot['std_mae'],\n",
    "            color=colors, edgecolor='black', capsize=8)\n",
    "axes[2].set_title('Mean MAE (Lower is better)', fontsize=16)\n",
    "axes[2].set_ylabel('MAE', fontsize=14)\n",
    "axes[2].set_ylim(bottom=0)\n",
    "\n",
    "# Ortak stil\n",
    "for ax in axes:\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=12)\n",
    "    ax.set_xlabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # suptitle için boşluk\n",
    "plt.savefig('model_performance_comparison_preferred.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# İSTATİSTİKSEL ANALİZ ve TASARIM KURALI DOĞRULAMA \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#  Yardımcı fonksiyonlar \n",
    "def numeric_cols(df):\n",
    "    return df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "def bootstrap_ci(stat_fn, y_true, y_pred, n_boot=5000, alpha=0.05, rng=42):\n",
    "    \"\"\"İkili sınıflandırma metrikleri için bootstrap GA (bias-corrected değil, hızlı).\"\"\"\n",
    "    rs = np.random.RandomState(rng)\n",
    "    stats = []\n",
    "    n = len(y_true)\n",
    "    idx = np.arange(n)\n",
    "    for _ in range(n_boot):\n",
    "        b = rs.choice(idx, size=n, replace=True)\n",
    "        stats.append(stat_fn(y_true[b], y_pred[b]))\n",
    "    lo = np.percentile(stats, 100*alpha/2)\n",
    "    hi = np.percentile(stats, 100*(1-alpha/2))\n",
    "    return float(np.mean(stats)), float(lo), float(hi)\n",
    "\n",
    "def metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    acc = (tp+tn) / cm.sum()\n",
    "    prec = tp / (tp+fp) if (tp+fp)>0 else 0.0\n",
    "    rec  = tp / (tp+fn) if (tp+fn)>0 else 0.0  # TPR / sensitivity\n",
    "    spec = tn / (tn+fp) if (tn+fp)>0 else 0.0 # TNR / specificity\n",
    "    f1   = (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    # Matthews correlation\n",
    "    denom = np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    mcc = ((tp*tn - fp*fn)/denom) if denom>0 else 0.0\n",
    "    cov = (tp+fp)/cm.sum()  # kuralın \"High\" dediği pay (coverage)\n",
    "    return dict(accuracy=acc, precision=prec, recall=rec, specificity=spec, f1=f1, mcc=mcc, coverage=cov)\n",
    "\n",
    "def plot_confusion(cm, labels=('Actual Low','Actual High'), preds=('Predicted Low','Predicted High'), title='Confusion Matrix'):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    totals = cm.sum()\n",
    "    pct = cm / totals * 100.0\n",
    "    annot = np.array([\n",
    "        [f\"TN\\n{tn}\\n{pct[0,0]:.1f}%\", f\"FP\\n{fp}\\n{pct[0,1]:.1f}%\"],\n",
    "        [f\"FN\\n{fn}\\n{pct[1,0]:.1f}%\", f\"TP\\n{tp}\\n{pct[1,1]:.1f}%\"]\n",
    "    ])\n",
    "    plt.figure(figsize=(7,6))\n",
    "    ax = sns.heatmap(cm, annot=annot, fmt='', cmap='Blues', cbar=False,\n",
    "                     xticklabels=preds, yticklabels=labels, linewidths=1, linecolor='black')\n",
    "    ax.set_xlabel('Predicted by Rules', fontsize=12)\n",
    "    ax.set_ylabel('Actual Performance', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 1) Veriyi yükle \n",
    "print(\"Loading dataset for statistical analysis...\")\n",
    "DATA_PATH = r\"C:\\Users\\kagan\\Downloads\\COMPO_ONLY_dataset.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns.\")\n",
    "\n",
    "#  2) Korelasyon ısı haritaları \n",
    "print(\"\\nGenerating Correlation Heatmaps (numeric columns only)...\")\n",
    "num_cols = numeric_cols(df)\n",
    "pearson_corr = df[num_cols].corr(method='pearson')\n",
    "spearman_corr = df[num_cols].corr(method='spearman')\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(pearson_corr, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=.5)\n",
    "plt.title('Pearson Correlation (numeric features)', fontsize=16)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(spearman_corr, annot=True, fmt=\".2f\", cmap='viridis', linewidths=.5)\n",
    "plt.title('Spearman Correlation (numeric features)', fontsize=16)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "#  3) Gerçek performans etiketleri \n",
    "performance_threshold = 9.0\n",
    "df['Actual Performance'] = np.where(df['dS'] >= performance_threshold, 1, 0)  # 1=High, 0=Low\n",
    "print(\"\\nCounts:\", df['Actual Performance'].value_counts().to_dict())\n",
    "\n",
    "#  4) Özellik dağılımları (kutu grafikleri) \n",
    "features_to_compare = [c for c in ['VEC','dHmix','dSmix'] if c in df.columns]\n",
    "print(\"\\nGenerating feature distribution plots...\")\n",
    "for feature in features_to_compare:\n",
    "    plt.figure(figsize=(9,6))\n",
    "    sns.boxplot(x='Actual Performance', y=feature, data=df, palette='viridis',\n",
    "                order=[0,1])\n",
    "    plt.title(f'Distribution of {feature} by Performance Group', fontsize=14)\n",
    "    plt.xlabel('Actual Performance (0=Low, 1=High)', fontsize=12)\n",
    "    plt.ylabel(feature, fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "#  5) Tasarım kuralı ve Confusion Matrix \n",
    "print(\"\\nEvaluating fixed compositional design rules...\")\n",
    "\n",
    "#  kural sınırı yaz \n",
    "rule_VEC  = (df['VEC']  >= 2.40) & (df['VEC']  <= 7.30) if 'VEC'  in df.columns else True\n",
    "rule_dSmix= (df['dSmix']>=11.50) & (df['dSmix']<=19.10) if 'dSmix'in df.columns else True\n",
    "rule_dHmix= (df['dHmix']>=-46.5) & (df['dHmix']<=-27.6)  if 'dHmix' in df.columns else True\n",
    "\n",
    "rule_mask = rule_VEC & rule_dSmix & rule_dHmix\n",
    "df['Predicted Performance'] = np.where(rule_mask, 1, 0)\n",
    "\n",
    "# Confusion matrix (label order: [Low, High] = [0,1])\n",
    "cm = confusion_matrix(df['Actual Performance'], df['Predicted Performance'], labels=[0,1])\n",
    "plot_confusion(cm, labels=('Actual Low','Actual High'), preds=('Predicted Low','Predicted High'),\n",
    "               title='Confusion Matrix for Compositional Design Rules')\n",
    "\n",
    "#  6) Metrikler + Bootstrap GA \n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "base_metrics = metrics_from_cm(cm)\n",
    "print(\"\\nPoint estimates:\")\n",
    "for k,v in base_metrics.items():\n",
    "    print(f\"  {k:>11s}: {v:.3f}\")\n",
    "\n",
    "# Bootstrap GA (accuracy, precision, recall, specificity, F1, MCC, coverage)\n",
    "y_true = df['Actual Performance'].to_numpy()\n",
    "y_pred = df['Predicted Performance'].to_numpy()\n",
    "\n",
    "def stat_acc(y, yhat):  return ((y==yhat).mean())\n",
    "def stat_prec(y, yhat): \n",
    "    tp = ((y==1)&(yhat==1)).sum(); fp = ((y==0)&(yhat==1)).sum()\n",
    "    return tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "def stat_rec(y, yhat):\n",
    "    tp = ((y==1)&(yhat==1)).sum(); fn = ((y==1)&(yhat==0)).sum()\n",
    "    return tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "def stat_spec(y, yhat):\n",
    "    tn = ((y==0)&(yhat==0)).sum(); fp = ((y==0)&(yhat==1)).sum()\n",
    "    return tn/(tn+fp) if (tn+fp)>0 else 0.0\n",
    "def stat_f1(y, yhat):\n",
    "    p = stat_prec(y,yhat); r = stat_rec(y,yhat); \n",
    "    return (2*p*r)/(p+r) if (p+r)>0 else 0.0\n",
    "def stat_mcc(y, yhat):\n",
    "    tp = ((y==1)&(yhat==1)).sum(); tn = ((y==0)&(yhat==0)).sum()\n",
    "    fp = ((y==0)&(yhat==1)).sum(); fn = ((y==1)&(yhat==0)).sum()\n",
    "    denom = np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    return ((tp*tn - fp*fn)/denom) if denom>0 else 0.0\n",
    "def stat_cov(y, yhat): return (yhat==1).mean()\n",
    "\n",
    "stats = {\n",
    "    \"accuracy\":   stat_acc,\n",
    "    \"precision\":  stat_prec,\n",
    "    \"recall_TPR\": stat_rec,\n",
    "    \"specificity\":stat_spec,\n",
    "    \"f1\":         stat_f1,\n",
    "    \"mcc\":        stat_mcc,\n",
    "    \"coverage\":   stat_cov\n",
    "}\n",
    "\n",
    "print(\"\\nBootstrap 95% CI (5000 resamples):\")\n",
    "for name, fn in stats.items():\n",
    "    mean, lo, hi = bootstrap_ci(lambda yt, yp: fn(yt, yp), y_true, y_pred, n_boot=5000, alpha=0.05, rng=123)\n",
    "    print(f\"  {name:>11s}: {mean:.3f}  (95% CI: {lo:.3f}–{hi:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
